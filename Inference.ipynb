{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1sBoro9JW7NEOf2bRxwQiAA7DHP2oBFLg",
      "authorship_tag": "ABX9TyNBwjlyM/WOMavipr6GY97/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shagufta24/Personalized_Writing_Assistant/blob/main/Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bKt-2QxwGPJ",
        "outputId": "e587a3f3-7de1-45e1-8020-edff4120d45d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U datasets scipy ipywidgets matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXcYpaeewtYK",
        "outputId": "b1e26bd7-edc6-4809-a6bf-587c9adc435c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: write).\n",
            "The token `my_token` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `my_token`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "BASE_MODEL_ID = \"mistralai/Mistral-7B-v0.3\"\n",
        "drive_path = \"/content/drive/MyDrive/cs598pen/mistral-finetune-reddit\""
      ],
      "metadata": {
        "id": "WKwI1mf6wzBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_path = drive_path + \"/base-model\"\n",
        "lora_adapter_path = drive_path + \"/checkpoint-125\""
      ],
      "metadata": {
        "id": "T3AKiD2FzVya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable logging\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "metadata": {
        "id": "Isz8sCmIIzMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained (\n",
        "    base_model_path,  # Mistral, same as before\n",
        "    quantization_config=bnb_config,  # Same quantization config as before\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW00PuVN5VnW",
        "outputId": "bf2bb966-d938-4712-981d-0d330e24e1ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_tokenizer = AutoTokenizer.from_pretrained(base_model_path, add_bos_token=True, trust_remote_code=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "hVImACRZ0Xxu",
        "outputId": "42088142-e3c0-447f-9470-1900eed7a264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'AutoTokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-23b9937090e2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_bos_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reload the fine-tuned model with LoRA adapter\n",
        "fine_tuned_model = PeftModel.from_pretrained(base_model, lora_adapter_path)"
      ],
      "metadata": {
        "id": "XX8h2dT-8Hs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score\n",
        "!pip install mauve\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "import mauve"
      ],
      "metadata": {
        "id": "0Gk55sFK6m32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# File paths\n",
        "TEST_FILE = \"/content/testing_data_reddit.jsonl\"\n",
        "\n",
        "# Load test data\n",
        "def load_test_data(file_path):\n",
        "    with open(file_path, \"r\") as f:\n",
        "        return [json.loads(line.strip()) for line in f]\n",
        "\n",
        "# Load test dataset\n",
        "test_data = load_test_data(TEST_FILE)"
      ],
      "metadata": {
        "id": "FQsmNEoS61Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper to generate responses\n",
        "def generate_response(model, tokenizer, prompt, max_new_tokens=50):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Prompt template\n",
        "def create_prompt(comment, user_name=\"1percentchick\"):\n",
        "    return f\"The following is a Reddit comment by {user_name}: {comment}\""
      ],
      "metadata": {
        "id": "bJ1cTP1469e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Metrics\n",
        "def evaluate_metrics(test_data, model, tokenizer, user_name=\"1percentchick\"):\n",
        "    bleu_scores = []\n",
        "    rouge_scores = []\n",
        "    references = []\n",
        "    predictions = []\n",
        "\n",
        "    rouge = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
        "\n",
        "    for example in test_data:\n",
        "        prompt = create_prompt(example[\"comment\"], user_name)\n",
        "        generated_response = generate_response(model, tokenizer, prompt)\n",
        "\n",
        "        # Reference text\n",
        "        reference = example[\"comment\"]\n",
        "        references.append(reference)\n",
        "        predictions.append(generated_response)\n",
        "\n",
        "        # BLEU score\n",
        "        bleu = sentence_bleu([reference.split()], generated_response.split())\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "        # ROUGE scores\n",
        "        rouge_result = rouge.score(reference, generated_response)\n",
        "        rouge_scores.append(rouge_result)\n",
        "\n",
        "    return bleu_scores, rouge_scores, references, predictions"
      ],
      "metadata": {
        "id": "vQkntAyC8vAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute MAUVE\n",
        "def compute_mauve(references, predictions):\n",
        "    mauve_result = mauve.compute_mauve(\n",
        "        p_text=predictions,\n",
        "        q_text=references,\n",
        "        device_id=0  # Set to -1 for CPU\n",
        "    )\n",
        "    return mauve_result.mauve"
      ],
      "metadata": {
        "id": "YYzp1uNF8xzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Base Model\n",
        "print(\"Evaluating Base Model...\")\n",
        "base_bleu, base_rouge, base_refs, base_preds = evaluate_metrics(test_data, base_model, tokenizer)\n",
        "\n",
        "# Evaluate Fine-Tuned Model\n",
        "print(\"Evaluating Fine-Tuned Model...\")\n",
        "fine_tuned_bleu, fine_tuned_rouge, fine_refs, fine_preds = evaluate_metrics(test_data, fine_tuned_model, tokenizer)\n",
        "\n",
        "# Compute MAUVE\n",
        "print(\"Computing MAUVE...\")\n",
        "base_mauve = compute_mauve(base_refs, base_preds)\n",
        "fine_tuned_mauve = compute_mauve(fine_refs, fine_preds)"
      ],
      "metadata": {
        "id": "wCLJj7QG5EQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display Results\n",
        "print(\"\\nBase Model Metrics:\")\n",
        "print(f\"Average BLEU: {sum(base_bleu) / len(base_bleu):.4f}\")\n",
        "print(f\"Average ROUGE: {sum([x['rougeL'].fmeasure for x in base_rouge]) / len(base_rouge):.4f}\")\n",
        "print(f\"MAUVE: {base_mauve:.4f}\")\n",
        "\n",
        "print(\"\\nFine-Tuned Model Metrics:\")\n",
        "print(f\"Average BLEU: {sum(fine_tuned_bleu) / len(fine_tuned_bleu):.4f}\")\n",
        "print(f\"Average ROUGE: {sum([x['rougeL'].fmeasure for x in fine_tuned_rouge]) / len(fine_tuned_rouge):.4f}\")\n",
        "print(f\"MAUVE: {fine_tuned_mauve:.4f}\")"
      ],
      "metadata": {
        "id": "jcbJMyb_82FC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}